{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df['same_security'] = df['same_security'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_data(\"dataset/Phase 1 - similarityCompetition/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "puncuation = set(string.punctuation)\n",
    "puncuation.add(\"-\")\n",
    "puncuation.add(\"'\")\n",
    "puncuation.add(\"\\\"\")\n",
    "snowball = SnowballStemmer('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(string):\n",
    "    stop_free = \" \".join([i for i in word_tokenize(string.lower()) if i not in stop_words])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in puncuation])\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    snowed = \" \".join(snowball.stem(word) for word in normalized.split())\n",
    "    \n",
    "    return snowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_desc_a = train_df['description_x'].apply(lambda x: clean(x))\n",
    "cleaned_desc_b = train_df['description_y'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4284"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = list(train_df['same_security']) + list(train_df['same_security'])\n",
    "len(pd.concat((cleaned_desc_a, cleaned_desc_b), ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelbecker/anaconda/envs/deeplearning/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "class TaggedDocumentIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in self.doc_list.items():\n",
    "            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n",
    " \n",
    "data = list(train_df['same_security']) + list(train_df['same_security'])\n",
    "sentences = TaggedDocumentIterator(pd.concat((cleaned_desc_a, cleaned_desc_b), ignore_index=True), data)\n",
    "\n",
    "model = Doc2Vec(vector_size=100, window=10, min_count=5, workers=11,alpha=0.025, epochs=20)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = TaggedDocument(words=cleaned_desc_a.iloc[0].split(), tags=0)\n",
    "two = TaggedDocument(words=cleaned_desc_b.iloc[0].split(), tags=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.5910154581069946), (0, 0.45649105310440063)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc_vec = model.infer_vector(cleaned_desc_b.iloc[1].split(), steps=50, alpha=0.25)\n",
    "model.docvecs.most_similar(positive=[new_doc_vec])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
